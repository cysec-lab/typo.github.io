import os
import re
import Levenshtein
from pyxdameraulevenshtein import damerau_levenshtein_distance
import pandas as pd
from difflib import SequenceMatcher
import difflib
from collections import defaultdict, Counter
import json
from typing import Dict, Tuple, Any, List, Set

# ====================================================================-
# ğŸ—„ï¸ 1. å®šæ•°ã¨æ±ç”¨ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° (å…ƒã®ã‚³ãƒ¼ãƒ‰ã‹ã‚‰æŠ½å‡º)
# ====================================================================

# ã‚­ãƒ¼ãƒœãƒ¼ãƒ‰é…åˆ—å®šæ•°
keyboard_adjacent = { 
    'q': 'wa', 'w': 'qase', 'e': 'wsdr', 'r': 'edft', 't': 'rfgy',
    'y': 'tghu', 'u': 'yhji', 'i': 'ujko', 'o': 'iklp', 'p': 'ol',
    'a': 'qws', 's': 'qwedazx', 'd': 'erfcsx', 'f': 'rtdgvcj',
    'g': 'tyfhvbn', 'h': 'yugjnb', 'j': 'uikhmnf', 'k': 'ijolm', 'l': 'okp',
    'z': 'asx', 'x': 'zsdc', 'c': 'xdfv', 'v': 'cfgb', 'b': 'vghn', 'n': 'bhjm', 'm': 'njk,',
    '.': ',/', ',': 'm.', '-': '^'
}
symmetric_key_pairs = [('f', 'j'), ('d', 'k'), ('s', 'l'), ('a', ';')]
homoglyph_pairs = [('1', 'l'), ('0', 'o'), ('i', 'l'), ('rn', 'm'), ('Ğ°', 'a'), ('b', 'd')] 
HOMOGLYPHS_FOR_GENERATOR = {'1': ['l'], '0': ['o'], 'i': ['l'], 'l': ['i'], 'r': ['m'], 'b': ['d'], 'd': ['b']} 
TLD_COSTS = {
    ".co.jp": "3,960å††/å¹´", 
    ".jp": "3,124å††/å¹´",
    ".com": "1,408å††/å¹´",
    ".net": "1,628å††/å¹´",
}

# ãƒ‰ãƒ¡ã‚¤ãƒ³éƒ¨æŠ½å‡º
def extract_domain(email):
    return email.split('@')[1] if '@' in email else ''

# ã‚­ãƒ¼ãƒœãƒ¼ãƒ‰éš£æ¥ãƒã‚§ãƒƒã‚¯
def keyboard_adjacent_check(c1, c2):
    return c1.lower() in keyboard_adjacent and c2.lower() in keyboard_adjacent[c1.lower()]

# å¯¾ç§°ã‚­ãƒ¼èª¤æ‰“ãƒã‚§ãƒƒã‚¯
def is_symmetric_mismatch(c1, c2):
    return any((c1 == a and c2 == b) or (c1 == b and c2 == a) for a, b in symmetric_key_pairs)

# ãƒ›ãƒ¢ã‚°ãƒªãƒ•èª¤æ‰“ãƒã‚§ãƒƒã‚¯
def is_visual_homoglyph(c1, c2):
    return any((c1 == a and c2 == b) or (c1 == b and c2 == a) for a, b in homoglyph_pairs)

# å˜ä¸€ã®è»¢ç½®ãƒŸã‚¹ã‚’è­˜åˆ¥
def get_transposed_pair(correct, typo):
    if damerau_levenshtein_distance(correct, typo) == 1 and Levenshtein.distance(correct, typo) > 1:
        for i in range(len(correct) - 1):
            if correct[i] == typo[i+1] and correct[i+1] == typo[i] and correct[:i] == typo[:i] and correct[i+2:] == typo[i+2:]:
                return (correct[i], correct[i+1])
    return None

# TLDãƒŸã‚¹ã®ç‰¹æ®Šãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è­˜åˆ¥
def is_tld_mismatch(correct_domain, typo_domain):
    if not ('.' in correct_domain and '.' in typo_domain): return False, None
    tld_pairs = [
        ('jp', 'co.jp'), ('co.jp', 'jp'), ('com', 'co.jp'), ('co.jp', 'com'), 
        ('ne.jp', 'co.jp'), ('co.jp', 'ne.jp'), ('go.jp', 'co.jp'), ('co.jp', 'go.jp')
    ]
    for c_pattern, t_pattern in tld_pairs:
        if correct_domain.endswith(c_pattern) and typo_domain.endswith(t_pattern):
            correct_base_part = correct_domain[:-len(c_pattern)] 
            typo_base_part = typo_domain[:-len(t_pattern)]
            if correct_base_part == typo_base_part:
                correct_tld_part = correct_domain[len(correct_base_part):]
                typo_tld_part = typo_domain[len(typo_base_part):]
                return True, f"{correct_tld_part} -> {typo_tld_part}"
    return False, None

# ç·¨é›†æ“ä½œã®åˆ†é¡ (åŸå› åˆ†é¡)
def classify_edit_ops_japanese(correct, typo):
    causes = set()
    if damerau_levenshtein_distance(correct, typo) == 1 and Levenshtein.distance(correct, typo) > 1: causes.add("å…¥åŠ›é †åºãƒŸã‚¹")
    ops = Levenshtein.editops(correct, typo)
    for tag, src_i, tgt_i in ops:
        c1 = correct[src_i] if src_i < len(correct) else ''
        c2 = typo[tgt_i] if tgt_i < len(typo) else ''
        if tag == 'replace':
            if keyboard_adjacent_check(c1, c2): causes.add("éš£æ¥ã‚­ãƒ¼èª¤æ‰“")
            elif is_symmetric_mismatch(c1, c2): causes.add("å·¦å³å¯¾ç§°ã‚­ãƒ¼èª¤æ‰“")
            elif is_visual_homoglyph(c1, c2): causes.add("ãƒ›ãƒ¢ã‚°ãƒªãƒ•ï¼ˆè¦–è¦šé¡ä¼¼æ–‡å­—ï¼‰")
            else: causes.add("ã‚¹ãƒšãƒ«ãƒŸã‚¹ï¼ˆèªçŸ¥ãƒŸã‚¹ï¼‰")
        elif tag == 'insert': causes.add("äºŒé‡å…¥åŠ›")
        elif tag == 'delete':
            if c1 == '.': causes.add("ãƒ‰ãƒƒãƒˆæŠœã‘")
            else: causes.add("å…¥åŠ›æ¼ã‚Œ")

    is_tld_m, _ = is_tld_mismatch(correct, typo)
    if is_tld_m:
        causes -= {"ã‚¹ãƒšãƒ«ãƒŸã‚¹ï¼ˆèªçŸ¥ãƒŸã‚¹ï¼‰", "äºŒé‡å…¥åŠ›", "å…¥åŠ›æ¼ã‚Œ", "ãƒ‰ãƒƒãƒˆæŠœã‘"}
        causes.add("TLDãƒŸã‚¹")
    return {'cause': 'ãƒ»'.join(sorted(causes))}

# å·®åˆ†éƒ¨åˆ†æŠ½å‡º
def extract_ngram_diffs(correct, typo):
    sm = difflib.SequenceMatcher(None, correct, typo)
    diffs = []
    for tag, i1, i2, j1, j2 in sm.get_opcodes():
        if tag == 'equal': continue
        src = correct[i1:i2] or 'ï¼ˆç©ºï¼‰'
        tgt = typo[j1:j2] or 'ï¼ˆç©ºï¼‰'
        diffs.append((src, tgt))
    return diffs

# DLè·é›¢1ã®å˜ä¸€æ“ä½œã‚’è­˜åˆ¥ï¼ˆäºˆæ¸¬ç”Ÿæˆç”¨ï¼‰
def identify_single_replacement(correct, typo):
    matcher = SequenceMatcher(None, correct, typo)
    ops = matcher.get_opcodes()
    replacements = [(correct[i1:i2], typo[j1:j2]) for tag, i1, i2, j1, j2 in ops if tag == 'replace']
    inserts = [(typo[j1:j2]) for tag, i1, i2, j1, j2 in ops if tag == 'insert']
    deletes = [(correct[i1:i2]) for tag, i1, i2, j1, j2 in ops if tag == 'delete']
    if len(replacements) == 1 and not inserts and not deletes and len(replacements[0][0]) == 1 and len(replacements[0][1]) == 1: return (replacements[0][0], replacements[0][1])
    if len(inserts) == 1 and not replacements and not deletes: return ('ï¼ˆç©ºï¼‰', inserts[0][0])
    if len(deletes) == 1 and not replacements and not inserts: return (deletes[0][0], 'ï¼ˆç©ºï¼‰')
    return ('', '')

# --- 2. ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã¨åˆ†æé–¢æ•° (å…ƒã®ã‚³ãƒ¼ãƒ‰ã‹ã‚‰æŠ½å‡º) ---

# TLDãƒŸã‚¹é›†è¨ˆã«å¯¾å¿œã—ãŸé‡ã¿è¨ˆç®—
def analyze_for_ranking(csv_path):
    df = pd.read_csv(csv_path)
    individual_rank_weights = defaultdict(dict)
    all_causes = []
    cause_diff_counter = defaultdict(Counter)

    for _, row in df.iterrows():
        correct = extract_domain(str(row['correct_address']))
        typo = extract_domain(str(row['input_address']))
        cause_field = str(row['cause'])
        causes = [c.strip() for c in cause_field.split('ãƒ»')]
        
        is_custom_handled = False
        row_causes = []

        for cause in causes:
            if cause == "TLDãƒŸã‚¹":
                is_tld_m, tld_diff_str = is_tld_mismatch(correct, typo)
                if is_tld_m:
                    cause_diff_counter[cause][tld_diff_str] += 1
                    is_custom_handled = True
            elif cause == "å…¥åŠ›é †åºãƒŸã‚¹":
                transposed_pair = get_transposed_pair(correct, typo)
                if transposed_pair:
                    c1, c2 = transposed_pair
                    key = f'{c1} {c2} -> {c2} {c1}'
                    cause_diff_counter[cause][key] += 1
                    is_custom_handled = True
            row_causes.append(cause)
        
        if is_custom_handled:
            all_causes.extend(row_causes)
            continue
            
        diffs = extract_ngram_diffs(correct, typo)
        all_causes.extend(row_causes) 

        for cause in causes:
            if cause in {"TLDãƒŸã‚¹", "å…¥åŠ›é †åºãƒŸã‚¹"}: continue
            for c1, c2 in diffs:
                cause_diff_counter[cause][(c1, c2)] += 1

    # W_individual ã®è¨ˆç®—
    total_typo_events = sum(sum(counter.values()) for counter in cause_diff_counter.values())
    
    for cause, counter in cause_diff_counter.items():
        for key, count in counter.items():
            rank_score = count / total_typo_events
            individual_rank_weights[cause][key] = rank_score
            
    # å¤§åˆ†é¡ã®å‰²åˆè¨ˆç®— (ãƒ¬ãƒãƒ¼ãƒˆç”¨)
    cause_counts = Counter(all_causes)
    total_major_events = sum(cause_counts.values())
    major_ratios = {k: round(v / total_major_events, 3) for k, v in cause_counts.items()}
            
    return major_ratios, individual_rank_weights

# ä½ç½®åˆ¥é »åº¦è¨ˆç®—
def calculate_positional_freqs(csv_path):
    df = pd.read_csv(csv_path)
    positional_data = defaultdict(lambda: defaultdict(Counter))

    for _, row in df.iterrows():
        correct = extract_domain(str(row['correct_address']))
        typo = extract_domain(str(row['input_address']))
        
        if "TLDãƒŸã‚¹" in str(row['cause']) or "å…¥åŠ›é †åºãƒŸã‚¹" in str(row['cause']): continue
        if damerau_levenshtein_distance(correct, typo) == 1 and Levenshtein.distance(correct, typo) == 1:
            ops = Levenshtein.editops(correct, typo)
            if len(ops) == 1:
                tag, src_i, tgt_i = ops[0]
                L = len(correct)
                
                if tag == 'insert':
                    char = typo[tgt_i].lower()
                    pos_absolute = src_i 
                elif tag == 'delete' or tag == 'replace':
                    char = correct[src_i].lower()
                    pos_absolute = src_i
                else: continue
                
                pos_relative_end = L - 1 - pos_absolute
                
                causes_for_classify = classify_edit_ops_japanese(correct, typo)['cause'].split('ãƒ»')
                cause = causes_for_classify[0] if causes_for_classify else 'ã‚¹ãƒšãƒ«ãƒŸã‚¹ï¼ˆèªçŸ¥ãƒŸã‚¹ï¼‰'
                
                if tag == 'delete' and char == '.': cause = "ãƒ‰ãƒƒãƒˆæŠœã‘"
                elif tag == 'insert': cause = "äºŒé‡å…¥åŠ›"
                elif tag == 'delete': cause = "å…¥åŠ›æ¼ã‚Œ"
                    
                positional_data[cause][char][pos_relative_end] += 1
    
    return positional_data

# TLDè²»ç”¨è¨ˆç®—
def extract_tld_and_cost(domain: str) -> str:
    for tld, cost in TLD_COSTS.items():
        if domain.endswith(tld): return cost
    return "è²»ç”¨ä¸æ˜"

# å…¨ä½“åŸå› å‰²åˆè¨ˆç®—
def get_cause_ratios(csv_path):
    df = pd.read_csv(csv_path)
    all_causes = []
    for cause in df["cause"].dropna():
        causes = [c.strip() for c in cause.split("ãƒ»")]
        all_causes.extend(causes)

    cause_counts = Counter(all_causes)
    total = sum(cause_counts.values())
    cause_ratios = {k: round(v / total, 3) for k, v in cause_counts.items()}
    return cause_ratios, cause_counts, total

# --- 3. JSONå¤‰æ›ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° ---
def convert_internal_keys_to_str(individual_weights: Dict[str, Dict[Tuple[Any, Any], float]]) -> Dict[str, Dict[str, float]]:
    converted_weights = {}
    for cause, inner_dict in individual_weights.items():
        converted_inner_dict = {}
        for key, score in inner_dict.items():
            if isinstance(key, tuple):
                key_str = "".join(key) 
            else:
                key_str = key 
            converted_inner_dict[key_str] = score
        converted_weights[cause] = converted_inner_dict
    return converted_weights

def convert_positional_freqs_to_json(positional_freqs: Dict) -> Dict:
    converted = {}
    for cause, char_data in positional_freqs.items():
        converted[cause] = {}
        for char, pos_counter in char_data.items():
            converted[cause][char] = dict(pos_counter) 
    return converted

# --- 4. ä¸­é–“ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ (JSONã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã«å¿…è¦) ---
def filter_domain_differences_with_mismatch(input_path, output_path, threshold=5):
    df = pd.read_csv(input_path)
    df["correct_domain"] = df["correct_address"].astype(str).apply(extract_domain)
    df["input_domain"] = df["input_address"].astype(str).apply(extract_domain)
    df["domain_edit_distance"] = df.apply(lambda row: damerau_levenshtein_distance(row["correct_domain"], row["input_domain"]), axis=1)
    filtered_df = df[(df["domain_edit_distance"] > 0) & (df["domain_edit_distance"] <= threshold)].reset_index(drop=True)
    filtered_df["edit_distance"] = filtered_df["domain_edit_distance"]
    filtered_df.to_csv(output_path, index=False)
    print(f"[INFO] 1. ãƒ‰ãƒ¡ã‚¤ãƒ³ã®é•ã„ã¨å·®åˆ†ã‚’å‡ºåŠ›ã—ã¾ã—ãŸ: {output_path}")

def append_typo_causes(input_csv_path, output_csv_path):
    df = pd.read_csv(input_csv_path)
    results = df.apply(lambda row: classify_edit_ops_japanese(extract_domain(str(row['correct_address'])), extract_domain(str(row['input_address']))), axis=1, result_type='expand')
    df['cause'] = results['cause']
    desired_columns = ['user_id', 'step_id', 'correct_address', 'input_address', 'edit_distance', 'cause']
    df = df[[col for col in desired_columns if col in df.columns]]
    df.to_csv(output_csv_path, index=False, encoding="utf-8-sig")
    print(f"[INFO] 2. åŸå› åˆ†é¡ã‚’ä»˜ä¸ã—ã¾ã—ãŸ: {output_csv_path}")

# --- 5. ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œãƒ–ãƒ­ãƒƒã‚¯ (JSONã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ) ---
if __name__ == "__main__":
    
    # å¿…é ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹è¨­å®š
    INPUT_FILE = "filtered_address.csv"
    DL4_FILTERED_FILE = "filtered_domain_typos_dl4.csv"
    CAUSES_CSV_FILE = "domaintypos_dl4_causes2.csv"
    DL_THRESHOLD = 4

    # 1 & 2. ä¸­é–“ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ (ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œç”¨)
    try:
        filter_domain_differences_with_mismatch(INPUT_FILE, DL4_FILTERED_FILE, DL_THRESHOLD)
        append_typo_causes(DL4_FILTERED_FILE, CAUSES_CSV_FILE)
    except FileNotFoundError:
        print(f"\n[è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼] å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ« ({INPUT_FILE}) ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚JSONã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        exit()
    
    # 3. åˆ†æã®å®Ÿè¡Œ: å€‹åˆ¥ãƒŸã‚¹ã®é‡ã¿ (W_individual) ã¨ä½ç½®åˆ¥é »åº¦ã®è¨ˆç®—
    major_weights, individual_rank_weights = analyze_for_ranking(CAUSES_CSV_FILE)
    positional_freqs = calculate_positional_freqs(CAUSES_CSV_FILE)
    _, _, total_events = get_cause_ratios(CAUSES_CSV_FILE)

    if not individual_rank_weights:
        print("\n[ã‚¨ãƒ©ãƒ¼] é‡ã¿ãƒ‡ãƒ¼ã‚¿ãŒè¨ˆç®—ã•ã‚Œãªã‹ã£ãŸãŸã‚ã€JSONã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
    else:
        # JSONå¤‰æ›ã®é©ç”¨
        converted_individual_weights = convert_internal_keys_to_str(individual_rank_weights)
        converted_positional_freqs = convert_positional_freqs_to_json(positional_freqs)
        total_dl1_count = sum(sum(c.values()) for char_data in positional_freqs.values() for c in char_data.values())
        if total_dl1_count == 0: total_dl1_count = 1

        OUTPUT_JSON_FILE = "data.json"
        
        web_data_export = {
            "individual_weights": converted_individual_weights, 
            "positional_freqs": converted_positional_freqs,
            "total_dl1_count": total_dl1_count,
            "K_POSITION_BOOST": 0.5,
            "TLD_COSTS": TLD_COSTS,
            
            "keyboard_adjacent": keyboard_adjacent,
            "symmetric_key_pairs": [list(pair) for pair in symmetric_key_pairs],
            "homoglyphs_for_generator": HOMOGLYPHS_FOR_GENERATOR
        }
        
        try:
            with open(OUTPUT_JSON_FILE, 'w', encoding='utf-8') as f:
                json.dump(web_data_export, f, indent=4, ensure_ascii=False)
            print(f"\n[INFO] Webç”¨ãƒ‡ãƒ¼ã‚¿ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†: {OUTPUT_JSON_FILE}")
        except Exception as e:
            print(f"\n[ERROR] JSONã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")